{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unidecode import unidecode\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descricao_preprocessing(input_text):\n",
    "    out_text = str(input_text).lower()\n",
    "    out_text = unidecode(out_text)\n",
    "    out_text = ' '.join([w for w in word_tokenize(\n",
    "        out_text) if w not in stopwords.words('portuguese')])\n",
    "    out_text = re.sub(r'[0-9]{5,}', '', out_text)\n",
    "    return out_text\n",
    "\n",
    "\n",
    "# function to combine similar tokens from corpus\n",
    "def combine_tokens(corpus):\n",
    "    tokenized_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
    "    # flatten the list\n",
    "    flattened_corpus = [\n",
    "        item for sublist in tokenized_corpus for item in sublist]\n",
    "    token_set = set(flattened_corpus)\n",
    "    token_set = [\n",
    "        token for token in token_set if token not in stopwords.words('portuguese')]\n",
    "    # check for tokens that are substring of other bigger tokens\n",
    "    token_set = [\n",
    "        token for token in token_set if not any(token in substring for substring in token_set if len(substring) > len(token))]\n",
    "    # for each sentence in the corpus, replace the tokens with the new supertokens\n",
    "    for i, sentence in enumerate(tokenized_corpus):\n",
    "        for j, token in enumerate(sentence):\n",
    "            for supertoken in token_set:\n",
    "                if token in supertoken:\n",
    "                    tokenized_corpus[i][j] = supertoken\n",
    "    # Join the tokens back to sentences\n",
    "    tokenized_corpus = [\n",
    "        ' '.join(sentence) for sentence in tokenized_corpus]\n",
    "    return tokenized_corpus\n",
    "\n",
    "\n",
    "# function to calculate cosine similarity between all sentences in the corpus and sum them up\n",
    "def calculate_similarity(corpus):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "    vectorized = vectorizer.fit_transform(corpus)\n",
    "    similarity_matrix = vectorized * vectorized.T\n",
    "    similarity_matrix = similarity_matrix.toarray()\n",
    "    similarity_matrix = np.sum(similarity_matrix, axis=1)\n",
    "    return similarity_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refri Coca Cola 2L\n",
    "Ref Coca 2L\n",
    "Refrigerante Coca Cola 2L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Carregando dados')\n",
    "data = pd.read_csv('nfedataEAN.csv',\n",
    "                    low_memory=False, encoding='utf-8')\n",
    "X = data.loc[:, 'produto_descricao']\n",
    "labels = data.loc[:, 'produto_codigo_ean'].value_counts().index.to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(descricao_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.loc[:, 'produto_codigo_ean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iniciando loop')\n",
    "# For loop to get each next 100 elements of the list\n",
    "for ean in labels.tolist():\n",
    "\n",
    "    mask = y == ean\n",
    "    current_y = y.loc[mask].copy()\n",
    "    corpus = X.loc[mask].tolist()\n",
    "    \n",
    "    rectified_corpus = combine_tokens(corpus)\n",
    "    similarity_matrix = calculate_similarity(rectified_corpus)\n",
    "    mean = np.mean(similarity_matrix)\n",
    "    Z = (similarity_matrix - mean)\n",
    "    where_not_zero = Z != 0\n",
    "    Z[where_not_zero] = Z[where_not_zero]/np.std(similarity_matrix)\n",
    "    remove = Z < -1\n",
    "    current_y.loc[remove] = ''\n",
    "    y.loc[mask] = current_y\n",
    "\n",
    "\n",
    "# unlabeled_data.to_csv('nfedataEAN_with_codigo_ean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "405abd14ff16bbcf8c693c2c7d3f7b5102466505fb51e0a9a6b851143bee253c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tce')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
